{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d232cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install torchaudio\n",
    "!pip install librosa\n",
    "!pip install jiwer\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd4c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "# Create a Wav2Vec2CTCTokenizer object with the generated vocabulary json with the required tokens\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45735eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Create a Wav2Vec2FeatureExtractor object for the audio\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True,\n",
    "                                             return_attention_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd398f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "# Create Wav2Vec2Processor object using the created feature_extractor and tokenizer\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef37ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset_train = load_from_disk(\"./processed_data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc56478",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = load_from_disk(\"./processed_data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e9ed0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1263b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(batch):\n",
    "    batch[\"speech\"] = librosa.resample(np.asarray(batch[\"speech\"]), 48_000, 16_000)\n",
    "    batch[\"sampling_rate\"] = 16_000\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72c63c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4114d4127e63443a85cd1c3a9a046e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=68849.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset_train.map(resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "224ece66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36330f5d238f484f8b8ccbd86207f7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17213.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_test = dataset_test.map(resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1dc5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "# Create Wav2Vec2Processor object using the created feature_extractor and tokenizer\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c738bf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\course_tbc\\4th year\\pp\\fyp\\project\\venv\\lib\\site-packages\\multiprocess\\pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"d:\\course_tbc\\4th year\\pp\\fyp\\project\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 176, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"d:\\course_tbc\\4th year\\pp\\fyp\\project\\venv\\lib\\site-packages\\datasets\\fingerprint.py\", line 397, in wrapper\n    out = func(self, *args, **kwargs)\n  File \"d:\\course_tbc\\4th year\\pp\\fyp\\project\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 1934, in _map_single\n    offset=offset,\n  File \"d:\\course_tbc\\4th year\\pp\\fyp\\project\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 1826, in apply_function_on_filtered_inputs\n    function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\n  File \"<ipython-input-17-e971a4e431bf>\", line 2, in prepare_dataset\nNameError: name 'processor' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-421461c900c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_train = dataset_train.map(prepare_dataset, remove_columns=dataset_train.column_names, batch_size=8, num_proc=4,\n\u001b[1;32m----> 2\u001b[1;33m                                   batched=True)\n\u001b[0m",
      "\u001b[1;32md:\\course_tbc\\4th year\\pp\\fyp\\project\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   1662\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Spawning {} processes\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwds_per_shard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1664\u001b[1;33m                 \u001b[0mtransformed_shards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1665\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Concatenating {} shards from multiprocessing\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_shards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\course_tbc\\4th year\\pp\\fyp\\project\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1662\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Spawning {} processes\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwds_per_shard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1664\u001b[1;33m                 \u001b[0mtransformed_shards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1665\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Concatenating {} shards from multiprocessing\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_shards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\course_tbc\\4th year\\pp\\fyp\\project\\venv\\lib\\site-packages\\multiprocess\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset_train.map(prepare_dataset, remove_columns=dataset_train.column_names, batch_size=8, num_proc=4,\n",
    "                                  batched=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
